
main-vars:
  terminal-rep-dim      : &word-rep-dim          128
  aux-tag-rep-dim       : &aux-tag-rep-dim         0
  word-recurrent-layers : &word-recurrent-layers   2
  aux-recurrent-layers  : &aux-recurrent-layers    0
  external-rep-dim      : &external-rep-dim        5


MLP:
  activations: ["tanh", "logsoftmax"]
  sizes:  [[*word-rep-dim, *aux-tag-rep-dim], *word-rep-dim, RESOURCE_OUT_TAGS_SIZE]

aux-tags-sequence-embedder:
  seq-emb-type: global
  recurrent-conf:
    bi-directional: true
    rnn-type: lstm
    in-dim: *aux-tag-rep-dim
    out-dim: *aux-tag-rep-dim
    layers: *aux-recurrent-layers
  sub-embedder-conf:
    seq-emb-type: local
    embedder-conf:
      emb-type: word-standard
      out-dim: *aux-tag-rep-dim
      w2i: RESOURCE_AUX2I

word-sequence-embedder:
  seq-emb-type: global
  recurrent-conf:
    bi-directional: true
    rnn-type: lstm
    in-dim: [*word-rep-dim, *external-rep-dim]
    out-dim: *word-rep-dim
    layers: *word-recurrent-layers
  sub-embedder-conf:
    seq-emb-type: combine
    combining-method: concat
    subembs:
      -
        seq-emb-type: BERT
        out-dim: *word-rep-dim
        normalize: true
        dropout: 0.2
      -
        seq-emb-type: External
        orig-dim: *external-rep-dim
        out-dim:  *external-rep-dim
        dropout: 0.2

trainer:
  type: Adam
  init-learning-rate: 0.002
  gradient-clipping: true
  decay-learning-rate: true
  mini-batch-size: 5
  reporting-frequency: 1000
  validation-frequency: -1

  precomputation-embeddings: true
  weight-decay: 0
  sparse-update: false
